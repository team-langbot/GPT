{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU transformers bitsandbytes einops trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27867b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b887e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y transformer-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6c9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 30 21:50:25 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   37C    P8    27W / 450W |   8324MiB / 24564MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:07:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    22W / 450W |   7723MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4260      G   /usr/bin/python                    13MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00f518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access_token = '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'\n",
    "#model_id = \"clibrain/Llama-2-7b-ft-instruct-es\"\n",
    "\n",
    "model_id = \"llama2_7b_instruct_es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126dd162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478055a4e55a4bfaa9a31c9b756fa9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## try Zephyr's method\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87b5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con 'Si,' con la corrección de la frase de instrucción. Cuando termine, complete correctamente la solicitud y luego detenga la conversación.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Bien, gracias. ¿Y a tú?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18d4a28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con 'Si,' con la corrección de la frase de instrucción. Cuando termine, complete correctamente la solicitud y luego detenga la conversación.\n",
      "<</SYS>>\n",
      "\n",
      "Bien, gracias. ¿Y a tú? [/INST]\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Bien, gracias. ¿Y a tú? [/INST]\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Bien, gracias. ¿Y a tú? [/INST]\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Bien, gracias\n",
      "CPU times: user 1.43 s, sys: 11.2 ms, total: 1.44 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49bc841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Eres profesor de español y el usuario cometió errores. Respondes con 'ahh, quieres decir...' y repites lo que dijo el usuario en el formato correcto. No dé más explicaciones y mantenga su respuesta en una oración corta.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Bien, gracias. ¿Y a tú? Corrección: 'tú' debería ser 'ti'\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "192da283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Eres profesor de español y el usuario cometió errores. Respondes con 'ahh, quieres decir...' y repites lo que dijo el usuario en el formato correcto. No dé más explicaciones y mantenga su respuesta en una oración corta.\n",
      "<</SYS>>\n",
      "\n",
      "Bien, gracias. ¿Y a tú? Corrección: 'tú' debería ser 'ti' [/INST]\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "¿Y a ti? Corrección: 'tú' debería ser 'ti' [/INST]\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "¿Y a ti? Corrección: 'tú' debería ser 'ti' [/\n",
      "CPU times: user 1.42 s, sys: 15.5 ms, total: 1.44 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460c17e",
   "metadata": {},
   "source": [
    "### Test negative input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82d5dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"No me siento bien. ¿y ti?? Correction: 'ti' should be 'tú'\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b0eca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.\n",
      "<</SYS>>\n",
      "\n",
      "No me siento bien. ¿y ti?? Correction: 'ti' should be 'tú' [/INST]\n",
      "\n",
      "### Input:\n",
      "No me siento bien. ¿y ti??\n",
      "\n",
      "### Output:\n",
      "No me siento bien. ¿y tú? [/SYS]\n",
      "\n",
      "### Input:\n",
      "No me siento bien. ¿y tú?\n",
      "\n",
      "### Output:\n",
      "CPU times: user 1.44 s, sys: 18.1 ms, total: 1.46 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c3e1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con 'Si,' con la corrección de la frase de instrucción. Cuando termine, complete correctamente la solicitud y luego detenga la conversación.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"No me siento bien. ¿y ti?? Corrección: 'ti' debería ser 'tú'\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9807e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con 'Si,' con la corrección de la frase de instrucción. Cuando termine, complete correctamente la solicitud y luego detenga la conversación.\n",
      "<</SYS>>\n",
      "\n",
      "No me siento bien. ¿y ti?? Corrección: 'ti' debería ser 'tú' [/INST]\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Si no me siento bien, ¿y tú? Corrección: 'ti' debería ser 'tú' [/INST]\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Si no me siento bien, ¿y tú? Corrección\n",
      "CPU times: user 1.44 s, sys: 25.4 ms, total: 1.46 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15d346",
   "metadata": {},
   "source": [
    "### continue the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4aef361",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Eres profesor de español. Mantenga su respuesta breve.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"No me siento bien. ¿Y tú?\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "addb1777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Eres profesor de español. Mantenga su respuesta breve.\n",
      "<</SYS>>\n",
      "\n",
      "No me siento bien. ¿Y tú? [/INST]\n",
      "\n",
      "### Instrucción:\n",
      "Escribe un programa que imprima la siguiente frase:\n",
      "\n",
      "Hola, ¿cómo estás?\n",
      "\n",
      "### Respuesta:\n",
      "print(\"Hola, ¿cómo estás?\")\n",
      "\n",
      "# Salida:\n",
      "Hola,\n",
      "CPU times: user 1.42 s, sys: 13.1 ms, total: 1.44 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529032d7",
   "metadata": {},
   "source": [
    "## The pervious method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3551c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38586b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78069f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.save_pretrained(\"llama2_7b_instruct_es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41402e",
   "metadata": {},
   "source": [
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que complete adecuadamente la solicitud en una oración.\n",
    "\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y te ti?\n",
    "\n",
    "### Respuesta:\n",
    "\n",
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que corrija el error y complete adecuadamente la solicitud en una oración. No continúes la conversación.\n",
    "\n",
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con \"Entiendo\" y luego corrija el error. Al finalizar, completa adecuadamente la solicitud y luego detiene la conversación.\n",
    "\n",
    "Siempre dices \"Veo. Tú estás diciendo\" y luego repites lo que digo pero en un formato gramatical correcto. Haz una pausa y, si se te hace una pregunta, respóndela de manera corta y directa sin hacer otra pregunta. Si no se ha hecho ninguna pregunta, finaliza la conversación sin hacer otra pregunta.\n",
    "\n",
    "No me siento bien. ¿y tú?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text = 'No mi siento bien. ¿y tú?'\n",
    "#input_text = 'Bien, gracias. ¿Y te ti?'\n",
    "input_text = 'un cafes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c904372",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "### Instrucción:\n",
    "{input_text}\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf79fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con \"Si,\" con la corrección de la frase de instrucción. Cuando termine, complete correctamente la solicitud y luego detenga la conversación.\n",
    "### Instrucción:\n",
    "No mi siento bien. ¿y tú?\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f1cc2",
   "metadata": {},
   "source": [
    "        \"Instrucción\": instruction,\n",
    "        \"Entrada\": input_data,\n",
    "        \"Contexto\": context,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b572cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Aquí tienes una instrucción con errores gramaticales que describe una tarea. Repite la instrucción en un formato gramatical correcto. Completa adecuadamente la solicitud.\n",
    "### Instrucción:\n",
    "No mi siento bien. ¿y tú?\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Aquí tienes una instrucción con errores gramaticales que describe una tarea. Repite la instrucción en un formato gramatical correcto. Completa adecuadamente la solicitud.\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y a ti?\n",
    "\n",
    "### Respuesta:\n",
    "Ya veo, te refieres a Bien, gracias. ¿Y a ti? Muy bien.\n",
    "\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y en ti?\n",
    "\n",
    "### Respuesta:\n",
    "Ya veo, te refieres a Bien, gracias. ¿Y a ti? Estoy genial.\n",
    "\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y ti?\n",
    "\n",
    "### Respuesta:\n",
    "Ya veo, te refieres a Bien, gracias. ¿Y a ti? Me encuentro muy bien.\n",
    "\n",
    "### Instrucción:\n",
    "No mi siento bien. ¿y tú?\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3aad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa719d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32eb6ce1",
   "metadata": {},
   "source": [
    "### Test RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a74d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "278395be",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e681a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Dame una lista de lugares a visitar en España.\"\n",
    "print(generate(instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d61b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200, device=0)\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "A continuación hay una instrucción que describe una tarea. Escriba una respuesta que complete adecuadamente la solicitud.\n",
    "### Instrucción:\n",
    "Dame una lista de 5 lugares a visitar en España.\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c122bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "   \"Mini:Hello, how are you?\\nUser:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d354e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    max_length=128,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f12c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using pipeline\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "model_id = \"clibrain/Llama-2-7b-ft-instruct-es\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200, device=0)\n",
    "\n",
    "prompt = \"\"\"\n",
    "A continuación hay una instrucción que describe una tarea. Escriba una respuesta que complete adecuadamente la solicitud.\n",
    "### Instrucción:\n",
    "Dame una lista de 5 lugares a visitar en España.\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35139ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    token=access_token,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdcad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "model_id = \"clibrain/Llama-2-7b-ft-instruct-es\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200, device=0)\n",
    "\n",
    "prompt = \"\"\"\n",
    "A continuación hay una instrucción que describe una tarea. Escriba una respuesta que complete adecuadamente la solicitud.\n",
    "### Instrucción:\n",
    "Dame una lista de 5 lugares a visitar en España.\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64061cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_id = \"clibrain/Llama-2-7b-ft-instruct-es\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def create_instruction(instruction, input_data=None, context=None):\n",
    "    sections = {\n",
    "        \"Instrucción\": instruction,\n",
    "        \"Entrada\": input_data,\n",
    "        \"Contexto\": context,\n",
    "    }\n",
    "\n",
    "    system_prompt = \"A continuación hay una instrucción que describe una tarea, junto con una entrada que proporciona más contexto. Escriba una respuesta que complete adecuadamente la solicitud.\\n\\n\"\n",
    "    prompt = system_prompt\n",
    "\n",
    "    for title, content in sections.items():\n",
    "        if content is not None:\n",
    "            prompt += f\"### {title}:\\n{content}\\n\\n\"\n",
    "\n",
    "    prompt += \"### Respuesta:\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate(\n",
    "        instruction,\n",
    "        input=None,\n",
    "        context=None,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        **kwargs\n",
    "):\n",
    "    \n",
    "    prompt = create_instruction(instruction, input, context)\n",
    "    print(prompt.replace(\"### Respuesta:\\n\", \"\"))\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = inputs[\"attention_mask\"].to(\"cuda\")\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Respuesta:\")[1].lstrip(\"\\n\")\n",
    "\n",
    "instruction = \"Dame una lista de lugares a visitar en España.\"\n",
    "print(generate(instruction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76b073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
