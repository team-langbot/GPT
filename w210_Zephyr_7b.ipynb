{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83932614",
   "metadata": {},
   "source": [
    "# The latest Zephyr with instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77dc46f",
   "metadata": {},
   "source": [
    "### primary English, but it works in Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace79b3a",
   "metadata": {},
   "source": [
    "### Instructions are in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU transformers bitsandbytes einops trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d52d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers from source - only needed for versions <= v4.34\n",
    "# pip install git+https://github.com/huggingface/transformers.git\n",
    "# pip install accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56473498",
   "metadata": {},
   "outputs": [],
   "source": [
    "## only do this when error occur during import\n",
    "#!pip uninstall -y transformer-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78ec459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 30 05:05:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   36C    P8    27W / 450W |    587MiB / 24564MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:07:00.0 Off |                  Off |\n",
      "|  0%   31C    P8    23W / 450W |     10MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "# load saved model to speed up the process\n",
    "model_name = 'zephyr_7b_beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a56e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519367f84fe1428aa62d42ff283387ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146d4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we save the model locally\n",
    "#pipe.save_pretrained(\"zephyr_7b_beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5c243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the tokenizer's chat template to format each message\n",
    "# see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2fbf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting?</s>\n",
      "<|assistant|>\n",
      "Matey, I be thinking yer askin' the wrong question! Helicopters be machines, not food. Humans can't eat machines. Yer askin' how many helicopters a human can fit in one place, like a hangar or a helipad. But if yer askin' about how many helicopters a human can operate, well, that's a different matter. The record for the most helicopters flown by a single pilot in one day is 17, set by a British Army Air Corps officer in 2000. But that's a feat of skill and endurance, not appetite!\n"
     ]
    }
   ],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "# <|system|>\n",
    "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de181a92",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ffb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Bien, gracias. ¿Y a tú?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63fa7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.</s>\n",
      "<|user|>\n",
      "Bien, gracias. ¿Y a tú?</s>\n",
      "<|assistant|>\n",
      "Bien, gracias. Quieres decir \"bien, gracias. Y a ti?\" (Ah, you mean, \"well, thank you. And you?\")\n",
      "\n",
      "Ahh, me gusta. Me llamo... (Ah, you mean, \"I like it.\n",
      "CPU times: user 1.47 s, sys: 24.6 ms, total: 1.49 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])\n",
    "# <|system|>\n",
    "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n",
    "#\n",
    "#  Correction: 'tú' should be 'ti'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c5941",
   "metadata": {},
   "source": [
    "### version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb638d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Bien, gracias. ¿Y a tú? Correction: 'tú' should be 'ti'\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb586d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.</s>\n",
      "<|user|>\n",
      "Bien, gracias. ¿Y a tú? Correction: 'tú' should be 'ti'</s>\n",
      "<|assistant|>\n",
      "Ah, realmente quieres decir \"ti\"? ¡Gracias!\n",
      "CPU times: user 466 ms, sys: 7.12 ms, total: 474 ms\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f276b5",
   "metadata": {},
   "source": [
    "### Test negative input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11df1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"No me siento bien. ¿y ti?? Correction: 'ti' should be 'tú'\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eb7e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a Spanish language teacher, and the user made mistakes. You respond with 'ahh, you mean,...' and repeat what the user said in the correct format. Don't further explain, and keep your response in one short sentence.</s>\n",
      "<|user|>\n",
      "No me siento bien. ¿y ti?? Correction: 'ti' should be 'tú'</s>\n",
      "<|assistant|>\n",
      "Ahh, quieres decir \"no me sientes bien. Y tú?\" Correcto: \"tú\" en lugar de \"ti\"\n",
      "CPU times: user 818 ms, sys: 0 ns, total: 818 ms\n",
      "Wall time: 817 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0169f3a",
   "metadata": {},
   "source": [
    "### continue the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0855d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Spanish tutor. Keep your respond short.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"No me siento bien. ¿Y tú?\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "413cc22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a Spanish tutor. Keep your respond short.</s>\n",
      "<|user|>\n",
      "No me siento bien. ¿Y tú?</s>\n",
      "<|assistant|>\n",
      "Buenas, gracias por preguntar. Me siento bien gracias. (Notice that I responded in Spanish, keeping it short as requested.)\n",
      "CPU times: user 852 ms, sys: 0 ns, total: 852 ms\n",
      "Wall time: 852 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=50, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d23505",
   "metadata": {},
   "source": [
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que complete adecuadamente la solicitud en una oración.\n",
    "\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y te ti?\n",
    "\n",
    "### Respuesta:\n",
    "\n",
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que corrija el error y complete adecuadamente la solicitud en una oración. No continúes la conversación.\n",
    "\n",
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con \"Entiendo\" y luego corrija el error. Al finalizar, completa adecuadamente la solicitud y luego detiene la conversación.\n",
    "\n",
    "Siempre dices \"Veo. Tú estás diciendo\" y luego repites lo que digo pero en un formato gramatical correcto. Haz una pausa y, si se te hace una pregunta, respóndela de manera corta y directa sin hacer otra pregunta. Si no se ha hecho ninguna pregunta, finaliza la conversación sin hacer otra pregunta.\n",
    "\n",
    "No me siento bien. ¿y tú?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text = 'No mi siento bien. ¿y tú?'\n",
    "#input_text = 'Bien, gracias. ¿Y te ti?'\n",
    "input_text = 'un cafes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29caf3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "### Instrucción:\n",
    "{input_text}\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "A continuación se muestra una instrucción con errores gramaticales que describe una tarea. Escriba una respuesta que comience con \"Si,\" con la corrección de la frase de instrucción. Cuando termine, complete correctamente la solicitud y luego detenga la conversación.\n",
    "### Instrucción:\n",
    "No mi siento bien. ¿y tú?\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf3f62",
   "metadata": {},
   "source": [
    "        \"Instrucción\": instruction,\n",
    "        \"Entrada\": input_data,\n",
    "        \"Contexto\": context,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Aquí tienes una instrucción con errores gramaticales que describe una tarea. Repite la instrucción en un formato gramatical correcto. Completa adecuadamente la solicitud.\n",
    "### Instrucción:\n",
    "No mi siento bien. ¿y tú?\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae674f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Aquí tienes una instrucción con errores gramaticales que describe una tarea. Repite la instrucción en un formato gramatical correcto. Completa adecuadamente la solicitud.\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y a ti?\n",
    "\n",
    "### Respuesta:\n",
    "Ya veo, te refieres a Bien, gracias. ¿Y a ti? Muy bien.\n",
    "\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y en ti?\n",
    "\n",
    "### Respuesta:\n",
    "Ya veo, te refieres a Bien, gracias. ¿Y a ti? Estoy genial.\n",
    "\n",
    "### Instrucción:\n",
    "Bien, gracias. ¿Y ti?\n",
    "\n",
    "### Respuesta:\n",
    "Ya veo, te refieres a Bien, gracias. ¿Y a ti? Me encuentro muy bien.\n",
    "\n",
    "### Instrucción:\n",
    "No mi siento bien. ¿y tú?\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "result = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature = 0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851270a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281e14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094cabfb",
   "metadata": {},
   "source": [
    "### Test RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8dd181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8df90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
