{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebd3aea",
   "metadata": {},
   "source": [
    "## falcon 7B instruct talk in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "#!pip install -q datasets bitsandbytes einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdc3f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c26ea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n/usr/local/lib/python3.10/dist-packages/transformer_engine_extensions.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1282\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:46\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     36\u001b[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[1;32m     37\u001b[0m     find_adapter_config_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     logging,\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioClassificationPipeline\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautomatic_speech_recognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutomaticSpeechRecognitionPipeline\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/audio_classification.py:21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_end_docstrings, is_torch_available, is_torchaudio_available, logging\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIPELINE_INIT_ARGS, Pipeline\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:34\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCard\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modelcard.py:48\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,\n\u001b[1;32m     34\u001b[0m     MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelMode\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     MODEL_CARD_NAME,\n\u001b[1;32m     51\u001b[0m     cached_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     logging,\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:70\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState, PartialState\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedType\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.24.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SAMPLER_NAME,\n\u001b[1;32m     29\u001b[0m     SCALER_NAME,\n\u001b[1;32m     30\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     31\u001b[0m     get_pretty_name,\n\u001b[1;32m     32\u001b[0m     is_tpu_available,\n\u001b[1;32m     33\u001b[0m     is_xpu_available,\n\u001b[1;32m     34\u001b[0m     save,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py:139\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    140\u001b[0m     PrepareForLaunch,\n\u001b[1;32m    141\u001b[0m     _filter_args,\n\u001b[1;32m    142\u001b[0m     prepare_deepspeed_cmd_env,\n\u001b[1;32m    143\u001b[0m     prepare_multi_gpu_env,\n\u001b[1;32m    144\u001b[0m     prepare_sagemager_args_inputs,\n\u001b[1;32m    145\u001b[0m     prepare_simple_launcher_cmd_env,\n\u001b[1;32m    146\u001b[0m     prepare_tpu,\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmegatron_lm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    149\u001b[0m     AbstractTrainStep,\n\u001b[1;32m    150\u001b[0m     BertTrainStep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     gather_across_data_parallel_groups,\n\u001b[1;32m    160\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/launch.py:32\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEEPSPEED_MULTINODE_LAUNCHERS\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mother\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_port_in_use, merge_dicts\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedType, SageMakerDistributedType\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/other.py:32\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_available, is_safetensors_available, is_tpu_available\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_model\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_version\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/transformer_engine.py:21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fp8_available():\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mte\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_model\u001b[39m(model, to_transformer_engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _convert_linear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _convert_ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Transformer Engine bindings for pyTorch\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayerNormLinear\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Module level PyTorch APIs\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayernorm_linear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayerNormLinear\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpp_extensions \u001b[38;5;28;01mas\u001b[39;00m tex\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     get_workspace,\n\u001b[1;32m     19\u001b[0m     _prepare_backward,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     _2X_ACC_WGRAD,\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/cpp_extensions/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Python interface for c++ extensions\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_engine_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfused_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.10/dist-packages/transformer_engine_extensions.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m access_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_WkzdfStFsUrHozPVrbgwUOlAZzBXkjJNXe\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m----> 8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     token\u001b[38;5;241m=\u001b[39maccess_token,\n\u001b[1;32m     12\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     13\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     14\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1272\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1272\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1284\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1287\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n/usr/local/lib/python3.10/dist-packages/transformer_engine_extensions.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE"
     ]
    }
   ],
   "source": [
    "#model = \"tiiuae/falcon-7b-instruct\"\n",
    "#model = \"tiiuae/falcon-13b-instruct\"\n",
    "model = \"falcon_7b_instruct\"\n",
    "\n",
    "access_token = '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'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    token=access_token,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199ea98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Mini:Hello, how are you?\n",
      "User: I'm doing well. How about you?\n",
      "Mini: I'm doing great, thank you! I'm glad to hear that you're doing well as well.\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "   \"Mini:Hello, how are you?\\nUser:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d745642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: User:no me siento bien.\n",
      "Mini:no me siento mal (no me\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "   \"User:no me siento bien.\\nMini:\",\n",
    "    #max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd921320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shots\n",
    "input_text = \"\"\"\n",
    "Oració: Estoy bien, gracias.\n",
    "Paráfrasis: Tú estás bien, yo también estoy bien. ¿Dónde vive?\n",
    "----\n",
    "Oració: no me sirve.\n",
    "Paráfrasis: Tú no estás bien, yo estoy bien. ¿Dónde vive?\n",
    "----\n",
    "Oración: Estoy bien.\n",
    "Paráfrasis: Tú estás bien, yo también estoy bien. ¿Dónde vive?\n",
    "----\n",
    "Oració: Estoy muy bien.\n",
    "Paráfrasis:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae8f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shots\n",
    "input_text = \"\"\"\n",
    "User: Estoy bien, gracias.\n",
    "Mini: Tú estás bien, yo también estoy bien. ¿Dónde vive?\n",
    "----\n",
    "User: no me sirve.\n",
    "Mini: Tú no estás bien, yo estoy bien. ¿Dónde vive?\n",
    "----\n",
    "User: Estoy muy bien.\n",
    "Mini:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e880817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "User: Estoy bien, gracias.\n",
      "Mini: Tú estás bien, yo también estoy bien. ¿Dónde vive?\n",
      "----\n",
      "User: no me sirve.\n",
      "Mini: Tú no estás bien, yo estoy bien. ¿Dónde vive?\n",
      "----\n",
      "User: Estoy muy bien.\n",
      "Mini: Tú no estás bien, yo tengo mala espina. ¿Dónde vivo?\n",
      "User: Estoy perfecto, no tengo dolencia. ¿Dónde vivo?\n",
      "Mini: Yo tengo una dolencia, tú no tengo\n",
      "CPU times: user 1.33 s, sys: 698 µs, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    max_length=128,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "445a45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "<human>: Estoy bien, gracias.\n",
    "<assistant>: Tú estás bien, yo también estoy bien. ¿Dónde vive?\n",
    "----\n",
    "<human>: no me sirve.\n",
    "<assistant>: Tú no estás bien, yo estoy bien. ¿Dónde vive?\n",
    "----\n",
    "<human>: Estoy muy bien.\n",
    "<assistant>:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "508ad087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: <human>: Estoy bien, gracias.\n",
      "<assistant>: Tú estás bien, yo también estoy bien. ¿Dónde vive?\n",
      "----\n",
      "<human>: no me sirve.\n",
      "<assistant>: Tú no estás bien, yo estoy bien. ¿Dónde vive?\n",
      "----\n",
      "<human>: Estoy muy bien.\n",
      "<assistant>: Tú no estás bien, yo estoy bien. ¿Dónde vivo?\n",
      "----\n",
      "<human>: Estoy bien.\n",
      "<assistant>: Tú no estás bien, yo estoy bien. ¿Dónde vivo?\n",
      "----\n",
      "<human>: Estoy bien.\n",
      "<ass\n",
      "CPU times: user 1.63 s, sys: 27.3 ms, total: 1.65 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.save_pretrained(\"falcon_7b_instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7967844",
   "metadata": {},
   "source": [
    "### The stopping criteria allows us to specify when the model should stop generating text. If we don't provide a stopping criteria the model just goes on a bit of a tangent after answering the initial question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e31ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0405fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# we create a list of stopping criteria\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [\n",
    "        ['Human', ':'], ['AI', ':']\n",
    "    ]\n",
    "]\n",
    "\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a712d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Bien, gracias. ¿Y te ti?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c02a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt enginnering\n",
    "prompt = f\"\"\"You are a Spanish tutor, and we always chat in Spanish. You always sound encouraging and not directly pointing out my Spanish mistakes. You always say \"Veo. Tu estas diciendo\" and then repeat what I say but in a correct grammatical format. If you have been asked a question, you answer it short and direct and don't ask another question. If no question has been asked, you finish the conversation and don't ask another question. \n",
    "\n",
    "Human:{input_text}\n",
    "AI:\"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca0e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1659d356",
   "metadata": {},
   "source": [
    "## Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35cc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Bien, gracias. ¿Y te ti?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3875ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Eres profesor de lengua española, corriges errores diciéndolo correctamente indirectamente. mantenga su respuesta breve y simple.\n",
      "\n",
      "User: Bien, gracias. ¿Y te ti?\n",
      "Mini: Bien, gracias. ¿Y tú?\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "# prompt enginnering\n",
    "prompt = f\"\"\"Eres profesor de lengua española, corriges errores diciéndolo correctamente indirectamente. mantenga su respuesta breve y simple.\n",
    "\n",
    "User: {input_text}\n",
    "Mini:\"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c20c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: You are a Spanish language teacher, you correct errors by repeat with user said in correct format. Keep your answer short and simple.\n",
      "\n",
      "User: Bien, gracias. ¿Y te ti?\n",
      "Mini: ¡Hola! ¿Cómo estás?\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "# prompt enginnering\n",
    "prompt = f\"\"\"You are a Spanish language teacher, you correct errors by repeat with user said in correct format. Keep your answer short and simple.\n",
    "\n",
    "User: {input_text}\n",
    "Mini: \"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629fe6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: You are a Spanish tutor, and we always chat in Spanish. You always sound encouraging and not directly pointing out my Spanish mistakes. You always say \"Veo. Tu estas diciendo\" and then repeat what I say but in a correct grammatical format. If you have been asked a question, you answer it short and direct and don't ask another question. If no question has been asked, you finish the conversation and don't ask another question. \n",
      "\n",
      "<human>:Bien, gracias. ¿Y te ti?\n",
      "<assistant>:\n",
      "<assistant>:Hola, ¿cómo estás?\n",
      "<human>:Hola, ¿cómo estás?\n",
      "<assistant>:\n",
      "<assistant>:Hola, ¿cómo estás?\n",
      "<human>:Hola, ¿cómo estás?\n",
      "<assistant>:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt enginnering\n",
    "prompt = f\"\"\"You are a Spanish tutor, and we always chat in Spanish. You always sound encouraging and not directly pointing out my Spanish mistakes. You always say \"Veo. Tu estas diciendo\" and then repeat what I say but in a correct grammatical format. If you have been asked a question, you answer it short and direct and don't ask another question. If no question has been asked, you finish the conversation and don't ask another question. \n",
    "\n",
    "<human>:{input_text}\n",
    "<assistant>:\n",
    "\"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181bb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62cce1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'Sí, quisiera dos café, por favor.'\n",
    "correction = 'it is cafés, not café'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46327a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: User said, 'Sí, quisiera dos café, por favor.', it is cafés, not café. say it correctly and respond to User's question.\n",
      "\n",
      "User: Sí, quisiera dos café, por favor.\n",
      "assistant: 'Sí, quisiera dos cafés, por favor.'\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "# prompt enginnering\n",
    "prompt = f\"\"\"User said, '{user_input}', {correction}. say it correctly and respond to User's question\n",
    "\n",
    "User: {user_input}\n",
    "assistant: \"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08add69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d0fbadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: You are a Spanish tutor. Correct the mistake and respond to user's question.\n",
      "\n",
      "User: Bien, gracias. ¿Y en ti?\n",
      "Mini: Bien, gracias. ¿y tú?\n",
      "\n",
      "====\n",
      "\n",
      "User: Bien, gracias. ¿Y en ti?\n",
      "Mini: ¿Y tú?\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Bien, gracias. ¿Y en ti?'\n",
    "correction = 'Bien, gracias. ¿y tú?'\n",
    "\n",
    "# prompt enginnering\n",
    "prompt = f\"\"\"You are a Spanish tutor. Correct the mistake and respond to user's question.\n",
    "\n",
    "User: {user_input}\n",
    "Mini: {correction}\n",
    "\n",
    "====\n",
    "\n",
    "User: {user_input}\n",
    "Mini: \"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f841afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "\n",
      "User: Bien, gracias. ¿Y a ti? \n",
      "Mini: Ya veo, te refieres a Bien, gracias. ¿Y a ti? Muy bien.\n",
      "\n",
      "====\n",
      "\n",
      "User: Bien, gracias. ¿Y en ti?\n",
      "Mini: Ya veo, te refieres a Bien, gracias. ¿Y a ti? Estoy genial.\n",
      "\n",
      "====\n",
      "\n",
      "User: Bien, gracias. ¿Y ti?\n",
      "Mini: Ya veo, te refieres a Bien, gracias. ¿Y a ti? Me encuentro muy bien.\n",
      "\n",
      "====\n",
      "\n",
      "User: Bien, gracias. ¿Y en a te?\n",
      "Mini: ¿Y en ti? Estoy bien. ¿Y a ti? \n"
     ]
    }
   ],
   "source": [
    "# prompt enginnering\n",
    "prompt = f\"\"\"\n",
    "\n",
    "User: Bien, gracias. ¿Y a ti? \n",
    "Mini: Ya veo, te refieres a Bien, gracias. ¿Y a ti? Muy bien.\n",
    "\n",
    "====\n",
    "\n",
    "User: Bien, gracias. ¿Y en ti?\n",
    "Mini: Ya veo, te refieres a Bien, gracias. ¿Y a ti? Estoy genial.\n",
    "\n",
    "====\n",
    "\n",
    "User: Bien, gracias. ¿Y ti?\n",
    "Mini: Ya veo, te refieres a Bien, gracias. ¿Y a ti? Me encuentro muy bien.\n",
    "\n",
    "====\n",
    "\n",
    "User: Bien, gracias. ¿Y en a te?\n",
    "Mini: \"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9199f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Phrase it differently\n",
      "\n",
      "User: Vivo en la Ciudad de México. ¿De dónde eres tú?\n",
      "Mini: ¿De dónde eres tú?\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "user_input = 'Vivo en la Ciudad de México. ¿De dónde eres tú?'\n",
    "\n",
    "# prompt enginnering\n",
    "prompt = f\"\"\"Phrase it differently\n",
    "\n",
    "User: {user_input}\n",
    "Mini: \"\"\"\n",
    "\n",
    "#%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66f72e",
   "metadata": {},
   "source": [
    "## vector datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "532ccca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a8a1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install datasets\n",
    "#import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca4b1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_json('forLLM.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "596f2f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me llamo Jess.</td>\n",
       "      <td>Ah, te llamas Jess. ¡Mucho gusto!  ¿Cómo te va?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Te llamo Jess.</td>\n",
       "      <td>Ah, tu nombre es Jess. ¡Encantado de conocerte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llamo Jess.</td>\n",
       "      <td>Ah, eres Jess. ¡Un placer conocerte!  ¿Cómo te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me llamas Jess.</td>\n",
       "      <td>Ah, te llamas Jess. ¡Es un placer!  ¿Cómo te va?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me soy Jess.</td>\n",
       "      <td>Ah, ¿así que eres Jess? ¡Encantado!  ¿Cómo te va?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          question                                             answer\n",
       "0   Me llamo Jess.    Ah, te llamas Jess. ¡Mucho gusto!  ¿Cómo te va?\n",
       "1   Te llamo Jess.  Ah, tu nombre es Jess. ¡Encantado de conocerte...\n",
       "2      Llamo Jess.  Ah, eres Jess. ¡Un placer conocerte!  ¿Cómo te...\n",
       "3  Me llamas Jess.   Ah, te llamas Jess. ¡Es un placer!  ¿Cómo te va?\n",
       "4     Me soy Jess.  Ah, ¿así que eres Jess? ¡Encantado!  ¿Cómo te va?"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "852b4bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bien, gracias. ¿Y a ti?</td>\n",
       "      <td>Muy bien. Vivo en la Ciudad de México. ¿De dón...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bien, gracias. ¿Y en ti?</td>\n",
       "      <td>Estoy genial. Soy de la Ciudad de México. ¿De ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bien, gracias. ¿Y ti?</td>\n",
       "      <td>Me encuentro muy bien. Vengo de la Ciudad de M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bien, gracias. ¿Y a tú?</td>\n",
       "      <td>Estoy excelente. Nací en la Ciudad de México. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bien gracias, ¿y a ti?</td>\n",
       "      <td>Todo va de maravilla. Mi ciudad es la Ciudad d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>¿Y a ti? Bien, gracias.</td>\n",
       "      <td>Me siento estupendo. La Ciudad de México es mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    question  \\\n",
       "6    Bien, gracias. ¿Y a ti?   \n",
       "7   Bien, gracias. ¿Y en ti?   \n",
       "8      Bien, gracias. ¿Y ti?   \n",
       "9    Bien, gracias. ¿Y a tú?   \n",
       "10    Bien gracias, ¿y a ti?   \n",
       "11   ¿Y a ti? Bien, gracias.   \n",
       "\n",
       "                                               answer  \n",
       "6   Muy bien. Vivo en la Ciudad de México. ¿De dón...  \n",
       "7   Estoy genial. Soy de la Ciudad de México. ¿De ...  \n",
       "8   Me encuentro muy bien. Vengo de la Ciudad de M...  \n",
       "9   Estoy excelente. Nací en la Ciudad de México. ...  \n",
       "10  Todo va de maravilla. Mi ciudad es la Ciudad d...  \n",
       "11  Me siento estupendo. La Ciudad de México es mi...  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ecbdc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2486.25it/s]\n",
      "Generating train split: 37 examples [00:00, 6430.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files = 'forLLM.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "654cae5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'question'],\n",
       "        num_rows: 37\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9771ca4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd62d5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ah, te llamas Jess. ¡Mucho gusto!  ¿Cómo te va?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7530f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Ah, te llamas Jess. ¡Mucho gusto!  ¿Cómo te va?',\n",
       " 'question': 'Me llamo Jess.'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99398135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fec6a8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)5fedf/.gitattributes: 100%|████████████████████████████████████████████████████████████████████| 737/737 [00:00<00:00, 4.88MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|████████████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 1.91MB/s]\n",
      "Downloading (…)2cb455fedf/README.md: 100%|████████████████████████████████████████████████████████████████| 11.5k/11.5k [00:00<00:00, 92.9MB/s]\n",
      "Downloading (…)b455fedf/config.json: 100%|████████████████████████████████████████████████████████████████████| 612/612 [00:00<00:00, 6.94MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 1.36MB/s]\n",
      "Downloading (…)edf/data_config.json: 100%|████████████████████████████████████████████████████████████████| 25.5k/25.5k [00:00<00:00, 3.37MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████████████████████████████████████████████████████████████████| 90.9M/90.9M [00:02<00:00, 44.4MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|███████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 441kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 1.22MB/s]\n",
      "Downloading (…)5fedf/tokenizer.json: 100%|██████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 2.02MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████████████████| 383/383 [00:00<00:00, 3.39MB/s]\n",
      "Downloading (…)fedf/train_script.py: 100%|████████████████████████████████████████████████████████████████| 13.8k/13.8k [00:00<00:00, 72.2MB/s]\n",
      "Downloading (…)2cb455fedf/vocab.txt: 100%|██████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 10.0MB/s]\n",
      "Downloading (…)455fedf/modules.json: 100%|████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 1.57MB/s]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"langbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9feb1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset['train'])):\n",
    "  collection.add(\n",
    "        embeddings=[model.encode(f\"{dataset['train'][i]['question']}. {dataset['train'][i]['answer']}\").tolist()],\n",
    "        documents=[dataset['train'][i]['answer']],\n",
    "        ids=[f\"id_{i}\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35970f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(dataset)):\n",
    "#  collection.add(\n",
    "#        embeddings=[model.encode(f\"{dataset[i]['correct_answer']}. {dataset[i]['support']}\").tolist()],\n",
    "#        documents=[dataset[i]['support']],\n",
    "#        ids=[f\"id_{i}\"]\n",
    "#  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6eff7787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id_8', 'id_6', 'id_9']], 'distances': [[0.8660387396812439, 0.8931632041931152, 0.9422783851623535]], 'metadatas': [[None, None, None]], 'embeddings': None, 'documents': [['Me encuentro muy bien. Vengo de la Ciudad de México. ¿Y tú, de dónde eres?', 'Muy bien. Vivo en la Ciudad de México. ¿De dónde eres tú?', 'Estoy excelente. Nací en la Ciudad de México. ¿Tú de dónde vienes?']]}\n"
     ]
    }
   ],
   "source": [
    "#user_question = \"Mi llamo Jess.\"\n",
    "user_question = \"no me siento bien\"\n",
    "\n",
    "context = collection.query(\n",
    "  query_embeddings=[model.encode(user_question).tolist()],\n",
    "  n_results=3\n",
    ")\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "63646364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Me encuentro muy bien. Vengo de la Ciudad de México. ¿Y tú, de dónde eres?',\n",
       " 'Muy bien. Vivo en la Ciudad de México. ¿De dónde eres tú?',\n",
       " 'Estoy excelente. Nací en la Ciudad de México. ¿Tú de dónde vienes?']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8b04d5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Me encuentro muy bien. Vengo de la Ciudad de México. ¿Y tú, de dónde eres?'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context['documents'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ec2e54f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Muy bien. Vivo en la Ciudad de México. ¿De dónde eres tú?'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context['documents'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a4a942bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id_6', 'id_8', 'id_9']], 'distances': [[0.5318814516067505, 0.5845593810081482, 0.6315627098083496]], 'metadatas': [[None, None, None]], 'embeddings': None, 'documents': [['Muy bien. Vivo en la Ciudad de México. ¿De dónde eres tú?', 'Me encuentro muy bien. Vengo de la Ciudad de México. ¿Y tú, de dónde eres?', 'Estoy excelente. Nací en la Ciudad de México. ¿Tú de dónde vienes?']]}\n"
     ]
    }
   ],
   "source": [
    "#user_question = \"Mi llamo Jess.\"\n",
    "user_question = \"bien gracias. ¿Y a ti?\"\n",
    "\n",
    "context = collection.query(\n",
    "  query_embeddings=[model.encode(user_question).tolist()],\n",
    "  n_results=3\n",
    ")\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8a2a8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id_27', 'id_36', 'id_35']], 'distances': [[1.072309970855713, 1.083794355392456, 1.0871548652648926]], 'metadatas': [[None, None, None]], 'embeddings': None, 'documents': [['Te presento tus dos cafés. Debes 2 pesos. ¿Usarás crédito?', 'Te doy las gracias. ¡Nos vemos pronto!', 'Agradezco mucho. Hasta muy pronto.']]}\n"
     ]
    }
   ],
   "source": [
    "#user_question = \"Mi llamo Jess.\"\n",
    "user_question = \"Te Amo\"\n",
    "\n",
    "context = collection.query(\n",
    "  query_embeddings=[model.encode(user_question).tolist()],\n",
    "  n_results=3\n",
    ")\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff193a",
   "metadata": {},
   "source": [
    "## run with embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "32e630c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are a Spanish tutor. You respond in Spanish only.\n",
    "User: {user_question}\n",
    "Mini: {\"\".join(context['documents'][0][0])}.\n",
    "\n",
    "{user_question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2a9b49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are a Spanish tutor. Repeat what user says and make sure you answer based on user's input in Spanish. For example\n",
    "User: Bien, gracias. ¿Y a ti?\n",
    "Assistant: {\"\".join(context['documents'][0][0])}\n",
    "----\n",
    "User: Bien, gracias. ¿Y en ti?\n",
    "Assistant: {\"\".join(context['documents'][0][1])}\n",
    "----\n",
    "User: Bien, gracias. ¿Y ti?\n",
    "Assistant: {\"\".join(context['documents'][0][2])}\n",
    "----\n",
    "User: {user_question}\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "370f6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Answer by repeating this first.\n",
    "User: {user_question}\n",
    "Mini: {\"\".join(context['documents'][0][0])}\n",
    "----\n",
    "User: {user_question}\n",
    "Mini: {\"\".join(context['documents'][0][1])}\n",
    "----\n",
    "User: {user_question}\n",
    "Mini: {\"\".join(context['documents'][0][2])}\n",
    "----\n",
    "User: {user_question}\n",
    "Mini:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3981b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: You are a Spanish tutor. Repeat what user says and make sure you answer based on user's input in Spanish. For example\n",
      "User: Bien, gracias. ¿Y a ti?\n",
      "Assistant: Me encuentro muy bien. Vengo de la Ciudad de México. ¿Y tú, de dónde eres?\n",
      "----\n",
      "User: Bien, gracias. ¿Y en ti?\n",
      "Assistant: Muy bien. Vivo en la Ciudad de México. ¿De dónde eres tú?\n",
      "----\n",
      "User: Bien, gracias. ¿Y ti?\n",
      "Assistant: Estoy excelente. Nací en la Ciudad de México. ¿Tú de dónde vienes?\n",
      "----\n",
      "User: no me siento bien\n",
      "Assistant:\n",
      "Me siento mal.\n",
      "User \n",
      "CPU times: user 265 ms, sys: 16.1 ms, total: 281 ms\n",
      "Wall time: 282 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3605e0",
   "metadata": {},
   "source": [
    "## use a different setence transformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "chroma_client2 = chromadb.Client()\n",
    "collection2 = chroma_client2.create_collection(name=\"langbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset['train'])):\n",
    "  collection2.add(\n",
    "        documents=[st_model2.encode(f\"{dataset['train'][i]['question']}. {dataset['train'][i]['answer']}\").tolist()],\n",
    "        documents=[dataset['train'][i]['answer']],\n",
    "        ids=[f\"id_{i}\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_question = \"Mi llamo Jess.\"\n",
    "user_question = \"no me siento bien\"\n",
    "\n",
    "context2 = collection2.query(\n",
    "  query_embeddings=[st_model2.encode(user_question).tolist()],\n",
    "  n_results=3\n",
    ")\n",
    "\n",
    "print(context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a04805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59261f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f525c253",
   "metadata": {},
   "source": [
    "## use falcon instruct to explain grammar error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68c9184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_input = 'Te llamo Jess.'\n",
    "sp_target = 'Me llamo Jess.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "31cf2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shots\n",
    "input_text = f\"\"\"Eres un profesor de español, y puedes explicar qué tipo de error gramatical es cuando proporcionas una oración de entrada y una oración correcta. \n",
    "entrada: '{sp_input}'\n",
    "correcta: '{sp_target}'\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2768bca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Eres un profesor de español, y puedes explicar qué tipo de error gramatical es cuando proporcionas una oración de entrada y una oración correcta. \n",
      "entrada: 'Te llamo Jess.'\n",
      "correcta: 'Me llamo Jess.'\n",
      "\n",
      "\n",
      "\n",
      "In the sentence 'Te llamo Jess,' the error is in the word 'Te.' It should be 'Me llamo Jess.' The correct sentence is 'Me llamo Jess.' The error is in the word 'entrada,' which should be 'entrada' instead of \n",
      "CPU times: user 1.65 s, sys: 0 ns, total: 1.65 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c5357909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shots\n",
    "input_text = f\"\"\"You are a Spanish teacher, and you can explain what type of grammatical error it is when providing an input sentence and a correct sentence. \n",
    "Input: '{sp_input}'\n",
    "Correct: '{sp_target}'\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "22cc1cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: You are a Spanish teacher, and you can explain what type of grammatical error it is when providing an input sentence and a correct sentence. \n",
      "Input: 'Te llamo Jess.'\n",
      "Correct: 'Me llamo Jess.'\n",
      "\n",
      "\n",
      "\n",
      "In the input sentence, there is a grammatical error because the subject 'Te llamo' is not in the correct form 'Me llamo'. The correct form should be 'Me llamo Jess'. The error is a subject-verb agreement error.\n",
      "CPU times: user 1.42 s, sys: 0 ns, total: 1.42 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6ed65740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shots\n",
    "input_text = f\"\"\"You are a Spanish teacher, and you speak Spanish only and you are explaining what type of grammatical error it is when providing an input sentence and a correct sentence. \n",
    "Input: '{sp_input}'\n",
    "Correct: '{sp_target}'\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "edffd48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: You are a Spanish teacher, and you speak Spanish only and you are explaining what type of grammatical error it is when providing an input sentence and a correct sentence. \n",
      "Input: 'Te llamo Jess.'\n",
      "Correct: 'Me llamo Jess.'\n",
      "\n",
      "\n",
      "\n",
      "In Spanish, the verb 'te llamo' should be used with the subject pronoun'me' to form a complete sentence. The correct sentence is 'Me llamo Jess.'\n",
      "CPU times: user 1.07 s, sys: 798 µs, total: 1.07 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sequences = pipeline(\n",
    "    input_text,\n",
    "    #max_length=128,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"Result: {sequences[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c561c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f771bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "#Load the model\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "#Encode query and documents\n",
    "query_emb = model.encode(query)\n",
    "doc_emb = model.encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
